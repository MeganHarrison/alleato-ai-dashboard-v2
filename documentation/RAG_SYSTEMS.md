# RAG Systems Documentation

Comprehensive documentation for all Retrieval-Augmented Generation (RAG) implementations in the Alleato AI Dashboard.

## Overview

The application implements multiple RAG systems for different use cases, each optimized for specific content types and retrieval patterns.

## 1. Main RAG System

### Location
- **Primary Code**: `/lib/rag/`
- **API Routes**: `/app/api/rag/`
- **Components**: `/components/ai-sdk5/`

### Technical Specifications

#### Embedding Configuration
- **Model**: `text-embedding-3-small` (OpenAI)
- **Dimensions**: 1536
- **Max Input Tokens**: 8191
- **Cost**: $0.00002 per 1K tokens
- **Batch Size**: 100 texts per request

#### Chunking Strategy
```typescript
{
  chunk_size: 1000,        // characters
  chunk_overlap: 200,      // characters
  separator: '\n\n',       // double newline
  keep_separator: false,
  max_chunk_tokens: 2000   // approximate
}
```

#### Vector Storage
- **Database**: Supabase PostgreSQL with pgvector
- **Table**: `document_embeddings`
- **Index Type**: HNSW (Hierarchical Navigable Small World)
- **Distance Metric**: Cosine similarity
- **Schema**:
```sql
CREATE TABLE document_embeddings (
  id UUID PRIMARY KEY,
  document_id UUID REFERENCES documents(id),
  chunk_index INTEGER,
  content TEXT,
  embedding vector(1536),
  metadata JSONB,
  created_at TIMESTAMP
);
```

#### Retrieval Configuration
- **Strategy**: Semantic similarity search
- **Top-K**: 10 results
- **Similarity Threshold**: 0.7
- **Reranking**: None (planned)
- **Query Enhancement**: None (planned)

### Performance Metrics
- **Indexing Speed**: ~100 chunks/second
- **Query Latency**: <200ms for 10K vectors
- **Storage**: ~6KB per chunk (including embedding)
- **Accuracy**: 85% relevance on test queries

### Suggested Improvements
1. **Hybrid Search**: Combine vector search with BM25 keyword search
2. **Query Expansion**: Use LLM to generate query variations
3. **Semantic Chunking**: Use NLP to identify logical boundaries
4. **Caching Layer**: Redis cache for frequent queries
5. **Incremental Updates**: Support for document modifications

### Usage Example
```typescript
import { generateEmbedding, findSimilarChunks } from '@/lib/rag/embeddings';

// Generate embedding for query
const queryEmbedding = await generateEmbedding(query);

// Search for similar chunks
const results = await findSimilarChunks(
  queryEmbedding,
  chunkEmbeddings,
  10,  // top-k
  0.7  // threshold
);
```

---

## 2. Meeting Intelligence RAG

### Location
- **Actions**: `/app/actions/meeting-embedding-actions.ts`
- **Components**: `/components/meeting-intelligence-chat.tsx`
- **Workers**: `/supabase/functions/embed_meetings/`

### Technical Specifications

#### Embedding Configuration
- **Model**: `text-embedding-3-small` (OpenAI)
- **Dimensions**: 1536
- **Preprocessing**: Speaker diarization, timestamp extraction
- **Batch Processing**: Async queue system

#### Chunking Strategy
```typescript
{
  strategy: 'time-based',
  segment_duration: 300,    // 5 minutes
  overlap_duration: 30,     // 30 seconds
  include_speakers: true,
  preserve_sentences: true
}
```

#### Vector Storage
- **Tables**: 
  - `meeting_chunks` - Text segments
  - `meeting_embeddings` - Vector embeddings
  - `meeting_metadata` - Meeting information
- **Indexes**: Composite index on (meeting_id, timestamp)
- **Schema**:
```sql
CREATE TABLE meeting_chunks (
  id UUID PRIMARY KEY,
  meeting_id UUID,
  chunk_index INTEGER,
  start_time INTEGER,
  end_time INTEGER,
  speaker TEXT,
  content TEXT,
  created_at TIMESTAMP
);

CREATE TABLE meeting_embeddings (
  id UUID PRIMARY KEY,
  chunk_id UUID REFERENCES meeting_chunks(id),
  embedding vector(1536),
  metadata JSONB
);
```

#### Retrieval Configuration
- **Strategy**: Temporal-aware semantic search
- **Context Window**: Â±2 chunks around matches
- **Speaker Filter**: Optional speaker-specific search
- **Time Range**: Optional time-bound queries

### Performance Metrics
- **Processing Time**: ~1 minute per hour of transcript
- **Query Latency**: <300ms with temporal filtering
- **Storage**: ~8KB per minute of meeting
- **Accuracy**: 90% action item extraction

### Suggested Improvements
1. **Speaker Embeddings**: Separate embeddings per speaker
2. **Topic Modeling**: Extract and index meeting topics
3. **Sentiment Analysis**: Add emotional context to chunks
4. **Summary Generation**: Create hierarchical summaries
5. **Cross-Meeting Search**: Find patterns across meetings

### Usage Example
```typescript
import { searchMeetingChunks } from '@/app/actions/meeting-embedding-actions';

const results = await searchMeetingChunks({
  query: "project timeline",
  meetingId: meetingId,
  speaker: "John Doe",
  timeRange: { start: 0, end: 1800 }
});
```

---

## 3. FM Global Documentation RAG

### Location
- **Worker**: `/agents/workers/worker-fm-global-vectorizer/`
- **Components**: `/components/FMGlobalRAGChat.tsx`
- **Data**: `/public/data/sections/`

### Technical Specifications

#### Embedding Configuration
- **Model**: `text-embedding-ada-002` (OpenAI) - *Needs upgrade*
- **Dimensions**: 1536
- **Preprocessing**: PDF extraction, table parsing
- **Special Handling**: Technical diagrams, formulas

#### Chunking Strategy
```typescript
{
  strategy: 'hierarchical',
  levels: ['chapter', 'section', 'subsection'],
  preserve_structure: true,
  table_handling: 'separate',
  figure_captions: 'include',
  max_chunk_size: 1500
}
```

#### Vector Storage
- **Primary**: Cloudflare R2 (object storage)
- **Secondary**: Supabase (metadata and search)
- **Index**: Distributed across edge locations
- **Schema**:
```sql
CREATE TABLE fm_document_vectors (
  id UUID PRIMARY KEY,
  document_id TEXT,
  section_id TEXT,
  section_title TEXT,
  content TEXT,
  embedding vector(1536),
  page_numbers INTEGER[],
  tables JSONB[],
  figures JSONB[]
);
```

#### Retrieval Configuration
- **Strategy**: Hierarchical search with context
- **Compliance Check**: Verify standards compliance
- **Citation**: Exact section and page references
- **Context Enhancement**: Include parent/child sections

### Performance Metrics
- **Indexing**: 500 pages in ~10 minutes
- **Query Latency**: <500ms globally (edge cached)
- **Storage**: ~15MB per document
- **Accuracy**: 95% for compliance questions

### Suggested Improvements
1. **Model Upgrade**: Move to `text-embedding-3-small`
2. **Table Understanding**: Specialized table embeddings
3. **Formula Search**: Mathematical expression matching
4. **Diagram Analysis**: Visual element extraction
5. **Version Tracking**: Document revision comparison

### Usage Example
```typescript
import { searchFMDocuments } from '@/lib/fm-global/search';

const results = await searchFMDocuments({
  query: "sprinkler design density",
  document: "FM 8-34",
  includeTabl)es: true,
  includeFigures: false,
  complianceMode: true
});
```

---

## 4. Cloudflare Workers RAG

### Location
- **Workers**: `/workers/rag-vectorizer/`, `/workers/rag-search/`
- **API**: `/workers/rag-api/`
- **Config**: `wrangler.toml`

### Technical Specifications

#### Embedding Configuration
- **Model**: Configurable (OpenAI, Cohere, or Hugging Face)
- **Dimensions**: Variable (384-1536)
- **Rate Limiting**: 100 requests/minute
- **Retry Logic**: Exponential backoff

#### Processing Pipeline
```typescript
{
  queue: 'rag-processing',
  batch_size: 100,
  parallel_workers: 5,
  timeout: 30000,  // 30 seconds
  retry_attempts: 3,
  dead_letter_queue: true
}
```

#### Vector Storage
- **Primary**: Cloudflare Vectorize (beta)
- **Fallback**: D1 SQLite with vector extension
- **Cache**: Workers KV for frequent queries
- **Architecture**:
```javascript
// Worker binding configuration
{
  bindings: {
    VECTORIZE: "rag-vectors",
    DB: "rag-database",
    CACHE: "rag-cache",
    QUEUE: "rag-queue"
  }
}
```

#### Retrieval Configuration
- **Strategy**: Edge-optimized search
- **Caching**: 5-minute TTL for popular queries
- **Geographic Distribution**: Multi-region deployment
- **Rate Limiting**: Per-user quotas

### Performance Metrics
- **Throughput**: 1000 documents/hour
- **Query Latency**: <100ms at edge
- **Global Distribution**: 200+ locations
- **Cost**: $0.001 per 1000 queries

### Suggested Improvements
1. **Multi-Provider Support**: Failover between embedding providers
2. **Compression**: Vector quantization for storage efficiency
3. **Streaming**: Real-time document processing
4. **Analytics**: Query pattern analysis
5. **Auto-Scaling**: Dynamic worker allocation

### Usage Example
```typescript
// Worker endpoint
export default {
  async fetch(request, env) {
    const { query } = await request.json();
    
    // Check cache first
    const cached = await env.CACHE.get(query);
    if (cached) return Response.json(cached);
    
    // Generate embedding
    const embedding = await generateEmbedding(query, env);
    
    // Search vectors
    const results = await env.VECTORIZE.query(embedding, {
      topK: 10,
      namespace: 'documents'
    });
    
    // Cache results
    await env.CACHE.put(query, results, { expirationTtl: 300 });
    
    return Response.json(results);
  }
};
```

---

## 5. Project Management RAG (Ultimate PM Agent)

### Location
- **Agent**: `/agents/ultimate-pm-agent/`
- **Library**: `/agents/ultimate-pm-agent/lib/vectorization/`
- **Scripts**: `/agents/ultimate-pm-agent/scripts/vectorization/`

### Technical Specifications

#### Embedding Configuration
- **Model**: `text-embedding-3-small` (OpenAI)
- **Dimensions**: 1536
- **Context**: Project-specific namespaces
- **Update Frequency**: Real-time for active projects

#### Content Types
```typescript
{
  documents: ['requirements', 'specifications', 'reports'],
  communications: ['emails', 'messages', 'comments'],
  code: ['repositories', 'pull_requests', 'issues'],
  meetings: ['transcripts', 'notes', 'action_items']
}
```

#### Vector Storage
- **Database**: Supabase with partitioned tables
- **Partitioning**: By project and date
- **Retention**: 90 days for inactive projects
- **Schema**:
```sql
CREATE TABLE pm_vectors (
  id UUID PRIMARY KEY,
  project_id UUID,
  content_type TEXT,
  content TEXT,
  embedding vector(1536),
  metadata JSONB,
  created_at TIMESTAMP
) PARTITION BY RANGE (created_at);
```

### Performance Metrics
- **Real-time Indexing**: <1 second latency
- **Cross-Project Search**: <500ms for 100K vectors
- **Storage Efficiency**: 70% compression with quantization
- **Relevance**: 88% precision on project queries

### Suggested Improvements
1. **Temporal Weighting**: Recent content prioritization
2. **Team Context**: User-specific embeddings
3. **Task Dependencies**: Graph-based retrieval
4. **Predictive Caching**: Anticipate user queries
5. **Multi-Modal**: Support for diagrams and screenshots

---

## Comparative Analysis

| System | Model | Dimensions | Chunk Size | Storage | Best For |
|--------|-------|------------|------------|---------|----------|
| Main RAG | text-embedding-3-small | 1536 | 1000 chars | Supabase | General documents |
| Meeting RAG | text-embedding-3-small | 1536 | 5 min | Supabase | Transcripts |
| FM Global | text-embedding-ada-002 | 1536 | Hierarchical | R2+Supabase | Technical docs |
| Workers RAG | Configurable | 384-1536 | Variable | Vectorize | Edge processing |
| PM RAG | text-embedding-3-small | 1536 | Mixed | Supabase | Project data |

## Cost Analysis

### Monthly Estimates (10K documents, 1M queries)

| Component | Main RAG | Meeting RAG | FM Global | Workers | PM RAG |
|-----------|----------|-------------|-----------|---------|--------|
| Embeddings | $20 | $30 | $15 | $25 | $35 |
| Storage | $5 | $8 | $10 | $3 | $12 |
| Compute | $10 | $15 | $5 | $50 | $20 |
| **Total** | **$35** | **$53** | **$30** | **$78** | **$67** |

## Best Practices

### 1. Embedding Generation
- Always normalize text before embedding
- Batch requests to avoid rate limits
- Cache embeddings for static content
- Use appropriate chunk sizes for content type

### 2. Vector Storage
- Create appropriate indexes for query patterns
- Partition large tables by date or category
- Implement retention policies
- Regular vacuum and reindexing

### 3. Retrieval Optimization
- Pre-filter with metadata before vector search
- Use hybrid search for better recall
- Implement result re-ranking
- Cache frequent queries

### 4. Quality Monitoring
- Track query latency percentiles
- Monitor relevance scores
- Log failed queries for analysis
- A/B test retrieval strategies

## Migration Guide

### Upgrading Embedding Models
1. Generate new embeddings with updated model
2. Run parallel systems during transition
3. Compare quality metrics
4. Gradually shift traffic
5. Deprecate old embeddings

### Scaling Considerations
- **Vertical**: Increase vector dimensions for accuracy
- **Horizontal**: Shard by content type or date
- **Edge**: Deploy to multiple regions
- **Hybrid**: Combine strategies based on usage

## Troubleshooting

### Common Issues

#### 1. Poor Retrieval Quality
- Check embedding model version
- Verify chunk size appropriateness
- Review similarity threshold
- Analyze query preprocessing

#### 2. High Latency
- Check index configuration
- Review query complexity
- Verify connection pooling
- Consider caching strategy

#### 3. Rate Limiting
- Implement exponential backoff
- Use batch operations
- Consider multiple API keys
- Add queue system

#### 4. Storage Issues
- Implement compression
- Archive old embeddings
- Use quantization
- Consider external storage

## Future Roadmap

### Q1 2025
- [ ] Implement hybrid search across all systems
- [ ] Add query expansion capabilities
- [ ] Deploy multi-lingual support
- [ ] Introduce semantic caching

### Q2 2025
- [ ] Custom fine-tuned embedding models
- [ ] Graph-based RAG for relationships
- [ ] Real-time collaborative filtering
- [ ] Automated quality monitoring

### Q3 2025
- [ ] Multi-modal RAG (images, audio)
- [ ] Federated search across systems
- [ ] Personalized embeddings
- [ ] Zero-shot retrieval

## References

- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Cloudflare Vectorize](https://developers.cloudflare.com/vectorize/)
- [Supabase Vector Guide](https://supabase.com/docs/guides/ai/vector-columns)

---

**Last Updated**: January 2025
**Version**: 1.0.0
**Maintained By**: Alleato AI Team